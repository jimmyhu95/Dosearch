// æ–‡ä»¶æ‰«æå™¨ - æ‰«ææœ¬åœ°æ–‡ä»¶å¤¹å¹¶å¤„ç†æ–‡æ¡£

import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import { nanoid } from 'nanoid';
import { db, initializeDatabase } from '../db';
import { documents, categories, documentCategories, keywords, scanHistory } from '../db/schema';
import { indexDocument, initializeIndex, type MeiliDocument } from '../search/meilisearch';
import { parseDocument, getFileType, SUPPORTED_EXTENSIONS } from '../parsers';
import { addVector } from '../search/vector-search';
import { getDocumentCategories, generateSummary, extractKeywords } from '../classifier';
import { CATEGORY_DEFINITIONS } from '../classifier/categories';
import { eq, ne, inArray, and, sql } from 'drizzle-orm';
import type { FileType, ScanHistory } from '@/types';

export interface ScanOptions {
  recursive?: boolean;
  includeHidden?: boolean;
  fileTypes?: string[];
  excludePatterns?: string[];
  onProgress?: (progress: ScanProgress) => void;
}

export interface ScanProgress {
  phase: 'scanning' | 'processing' | 'indexing' | 'completed';
  totalFiles: number;
  processedFiles: number;
  currentFile?: string;
  newFiles: number;
  updatedFiles: number;
  errors: string[];
}

export interface ScanResult {
  scanId: string;
  totalFiles: number;
  processedFiles: number;
  newFiles: number;
  updatedFiles: number;
  errors: string[];
  duration: number;
}

/**
 * è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
 */
async function calculateFileHash(filePath: string): Promise<string> {
  const buffer = await fs.readFile(filePath);
  return crypto.createHash('md5').update(buffer).digest('hex');
}

/**
 * æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ‰«æ
 */
function shouldScanFile(
  filePath: string,
  options: ScanOptions
): boolean {
  const fileName = path.basename(filePath);
  const ext = path.extname(filePath).toLowerCase();

  // æ£€æŸ¥æ˜¯å¦ä¸ºéšè—æ–‡ä»¶
  if (!options.includeHidden && fileName.startsWith('.')) {
    return false;
  }

  // æ£€æŸ¥æ–‡ä»¶ç±»å‹
  if (options.fileTypes && options.fileTypes.length > 0) {
    if (!options.fileTypes.includes(ext)) {
      return false;
    }
  } else {
    // é»˜è®¤åªæ‰«ææ”¯æŒçš„æ–‡ä»¶ç±»å‹
    if (!SUPPORTED_EXTENSIONS.includes(ext)) {
      return false;
    }
  }

  // æ£€æŸ¥æ’é™¤æ¨¡å¼
  if (options.excludePatterns) {
    for (const pattern of options.excludePatterns) {
      if (filePath.includes(pattern) || fileName.match(new RegExp(pattern))) {
        return false;
      }
    }
  }

  return true;
}

// æ”¯æŒçš„æ‰©å±•ååˆ—è¡¨ï¼ˆå‰ç½®è¿‡æ»¤ï¼Œæå‡æ€§èƒ½ï¼‰
const SUPPORTED_EXTS = new Set(SUPPORTED_EXTENSIONS);

export async function getFilesRecursively(dirPath: string): Promise<string[]> {
  let results: string[] = [];
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });

    for (const entry of entries) {
      // å¿½ç•¥éšè—æ–‡ä»¶å’Œç³»ç»Ÿç›®å½•
      if (entry.name.startsWith('.') || entry.name === 'node_modules') continue;

      const fullPath = path.join(dirPath, entry.name);

      if (entry.isDirectory()) {
        // ã€å…³é”®ã€‘ï¼šå¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œç­‰å¾…é€’å½’å®Œæˆå¹¶åˆå¹¶ç»“æœ
        const subFiles = await getFilesRecursively(fullPath);
        results = results.concat(subFiles);
      } else {
        // å¦‚æœæ˜¯æ–‡ä»¶ï¼Œæ£€æŸ¥æ‰©å±•åæ˜¯å¦åœ¨æ”¯æŒåˆ—è¡¨ä¸­
        const ext = path.extname(entry.name).toLowerCase();
        if (SUPPORTED_EXTS.has(ext)) {
          results.push(fullPath);
        }
      }
    }
  } catch (error) {
    console.error(`[Scanner] æ— æ³•è¯»å–è·¯å¾„æˆ–æƒé™æ‹’ç»: ${dirPath}`, error);
  }
  return results;
}

/**
 * åˆå§‹åŒ–/åŒæ­¥é»˜è®¤åˆ†ç±»ï¼ˆå¯å¤šæ¬¡è°ƒç”¨ï¼Œå¹‚ç­‰ï¼‰
 */
export async function initializeCategories(): Promise<void> {
  for (const catDef of CATEGORY_DEFINITIONS) {
    try {
      await db.insert(categories).values({
        id: catDef.id,
        name: catDef.name,
        slug: catDef.slug,
        description: catDef.description,
        icon: catDef.icon,
        color: catDef.color,
        sortOrder: CATEGORY_DEFINITIONS.indexOf(catDef),
      }).onConflictDoNothing();
    } catch {
      // åˆ†ç±»å¯èƒ½å·²å­˜åœ¨
    }
  }

  // æ¸…ç†å†å²é”™è¯¯ï¼šåˆ é™¤éå›¾ç‰‡æ–‡ä»¶è¢«è¯¯å½’å…¥ image åˆ†ç±»çš„è®°å½•ï¼ˆä¸¤æ­¥ï¼Œé¿å…å­æŸ¥è¯¢å…¼å®¹æ€§é—®é¢˜ï¼‰
  try {
    const nonImageDocs = await db
      .select({ id: documents.id })
      .from(documents)
      .where(ne(documents.fileType, 'image'));
    if (nonImageDocs.length > 0) {
      const ids = nonImageDocs.map((d) => d.id);
      await db
        .delete(documentCategories)
        .where(and(eq(documentCategories.categoryId, 'image'), inArray(documentCategories.documentId, ids)));
    }
  } catch (e) {
    console.warn('Category cleanup error:', e);
  }
}

/**
 * å¤„ç†å•ä¸ªæ–‡ä»¶
 */
async function processFile(
  filePath: string,
  existingDoc?: { id: string; fileHash: string }
): Promise<{ isNew: boolean; isUpdated: boolean; error?: string }> {
  try {
    const stats = await fs.stat(filePath);
    const fileHash = await calculateFileHash(filePath);

    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœªä¿®æ”¹
    if (existingDoc && existingDoc.fileHash === fileHash) {
      return { isNew: false, isUpdated: false };
    }

    const ext = path.extname(filePath).toLowerCase();
    const fileType = getFileType(filePath) as FileType;

    // â”€â”€â”€ OFD ç¡¬æ‹¦æˆªï¼šè·³è¿‡è§£æä¸åˆ†ç±»ï¼Œç›´æ¥å½’å…¥æŠ¥é”€æ–‡ä»¶ â”€â”€â”€
    if (ext === '.ofd') {
      const ofdTitle = path.basename(filePath, ext);
      const ofdSummary = 'ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«ä¸º OFD ç”µå­å‘ç¥¨æ–‡ä»¶';
      const ofdCategories = [{ categoryId: 'reimbursement', categoryName: 'æŠ¥é”€æ–‡ä»¶', confidence: 1 }];

      const docId = existingDoc?.id || nanoid();
      const now = new Date().toISOString();

      if (existingDoc) {
        await db.update(documents)
          .set({
            title: ofdTitle,
            content: '',
            summary: ofdSummary,
            fileHash,
            modifiedAt: now,
            indexedAt: now,
            metadata: JSON.stringify({ format: 'OFD' }),
          })
          .where(eq(documents.id, existingDoc.id));
        await db.delete(documentCategories).where(eq(documentCategories.documentId, existingDoc.id));
        await db.delete(keywords).where(eq(keywords.documentId, existingDoc.id));
      } else {
        await db.insert(documents).values({
          id: docId,
          title: ofdTitle,
          filePath,
          fileType,
          fileSize: stats.size,
          content: '',
          summary: ofdSummary,
          createdAt: now,
          modifiedAt: now,
          indexedAt: now,
          fileHash,
          metadata: JSON.stringify({ format: 'OFD' }),
        });
      }

      for (const cat of ofdCategories) {
        await db.insert(documentCategories).values({
          documentId: docId,
          categoryId: cat.categoryId,
          confidence: cat.confidence,
        });
      }

      const meiliDoc: MeiliDocument = {
        id: docId,
        title: ofdTitle,
        content: '',
        summary: ofdSummary,
        fileType,
        filePath,
        categories: ofdCategories.map(c => c.categoryId),
        categoryNames: ofdCategories.map(c => c.categoryName),
        keywords: [],
        createdAt: new Date(now).getTime(),
        modifiedAt: new Date(now).getTime(),
        fileSize: stats.size,
      };

      try { await indexDocument(meiliDoc); } catch (e) {
        console.warn('Failed to index OFD document to MeiliSearch:', e);
      }

      return { isNew: !existingDoc, isUpdated: !!existingDoc };
    }

    // â”€â”€â”€ å¸¸è§„æ–‡ä»¶å¤„ç† â”€â”€â”€
    const parsed = await parseDocument(filePath);

    // ç”Ÿæˆæ‘˜è¦ (å¿…é¡»åŠ ä¸Š await)
    const summary = await generateSummary(parsed.content, 300);

    // åˆ†ç±»æ–‡æ¡£ï¼šå›¾ç‰‡å½’å›¾ç‰‡ï¼ŒExcelå½’æŠ¥è¡¨ï¼Œå…¶ä½™èµ°åˆ†ç±»å™¨ (ä½¿ç”¨ ext åˆ¤æ–­ï¼Œå¹¶åŠ ä¸Š await)
    const docCategories = fileType === 'image'
      ? [{ categoryId: 'image', categoryName: 'å›¾ç‰‡', confidence: 1 }]
      : (ext === '.xlsx' || ext === '.xls')
        ? [{ categoryId: 'report', categoryName: 'æŠ¥è¡¨', confidence: 1 }]
        : await getDocumentCategories(parsed.content, parsed.title);

    // æå–å…³é”®è¯ (å¿…é¡»åŠ ä¸Š await)
    const docKeywords = await extractKeywords(parsed.content, 20, 2);

    const docId = existingDoc?.id || nanoid();
    const now = new Date().toISOString();

    // ä¿å­˜æˆ–æ›´æ–°æ–‡æ¡£
    if (existingDoc) {
      await db.update(documents)
        .set({
          title: parsed.title,
          content: parsed.content,
          summary,
          fileHash,
          modifiedAt: now,
          indexedAt: now,
          metadata: parsed.metadata, // ğŸ’¡ ä¿®å¤ï¼šå»æ‰ JSON.stringifyï¼Œè®© Drizzle åº•å±‚è‡ªåŠ¨å¤„ç†
        })
        .where(eq(documents.id, existingDoc.id));

      // åˆ é™¤æ—§çš„åˆ†ç±»å’Œå…³é”®è¯
      await db.delete(documentCategories).where(eq(documentCategories.documentId, existingDoc.id));
      await db.delete(keywords).where(eq(keywords.documentId, existingDoc.id));
    } else {
      await db.insert(documents).values({
        id: docId,
        title: parsed.title,
        filePath,
        fileType,
        fileSize: stats.size,
        content: parsed.content,
        summary,
        createdAt: now,
        modifiedAt: now,
        indexedAt: now,
        fileHash,
        metadata: parsed.metadata, // ğŸ’¡ ä¿®å¤ï¼šå»æ‰ JSON.stringify
      });
    }

    // ä¿å­˜åˆ†ç±»å…³è”
    for (const cat of docCategories) {
      // ğŸ’¡ ç»ˆæé˜²å¾¡ï¼šå…¼å®¹ AI è¿”å›çš„ id å’Œ æˆ‘ä»¬ç¡¬ç¼–ç è¿”å›çš„ categoryId
      const finalCategoryId = cat.categoryId || (cat as any).id || 'other';
      await db.insert(documentCategories).values({
        documentId: docId,
        categoryId: finalCategoryId,
        confidence: cat.confidence || 1,
      });
    }

    // ä¿å­˜å…³é”®è¯ (å…¼å®¹ä¿®å¤ç‰ˆä¿æŒä¸å˜)
    for (const kw of docKeywords) {
      const keywordText = typeof kw === 'string' ? kw : kw.keyword;
      const keywordWeight = typeof kw === 'string' ? 1.0 : (kw.weight || 1.0);

      if (!keywordText) continue;

      await db.insert(keywords).values({
        id: nanoid(),
        documentId: docId,
        keyword: keywordText,
        weight: keywordWeight,
      });
    }

    // ç´¢å¼•åˆ°æœç´¢å¼•æ“
    const meiliDoc: MeiliDocument = {
      id: docId,
      title: parsed.title,
      content: parsed.content,
      summary,
      fileType,
      filePath,
      categories: docCategories.map(c => c.categoryId || (c as any).id || 'other'),
      categoryNames: docCategories.map(c => c.categoryName || (c as any).name || 'å…¶ä»–'),
      keywords: docKeywords.map(k => typeof k === 'string' ? k : k.keyword).filter(Boolean),
      createdAt: new Date(now).getTime(),
      modifiedAt: new Date(now).getTime(),
      fileSize: stats.size,
    };

    try {
      await indexDocument(meiliDoc);
    } catch (error) {
      console.warn('Failed to index document to MeiliSearch:', error);
    }

    // æ·»åŠ åˆ°å‘é‡æœç´¢
    try {
      await addVector(docId, `${parsed.title} ${parsed.content}`, {
        title: parsed.title,
        filePath,
        fileType,
      });
    } catch (error) {
      console.warn('Failed to add document to vector store:', error);
    }

    return {
      isNew: !existingDoc,
      isUpdated: !!existingDoc,
    };
  } catch (error) {
    // ğŸš¨ æ ¸å¿ƒå¤§æ‹›ï¼šå¼ºè¡ŒæŠŠè¢«åæ‰çš„é”™è¯¯å¤§å­—æ‰“å°åˆ°ä½ çš„ Node.js é»‘æ¡†æ§åˆ¶å°ä¸Šï¼
    console.error(`\nâŒ [ç»ˆææ’æŸ¥] æ–‡ä»¶å¤„ç†åœ¨æŸä¸€æ­¥å´©æºƒäº†ï¼æ–‡ä»¶: ${filePath}`);
    console.error(error);
    console.error(`-----------------------------------------\n`);

    return {
      isNew: false,
      isUpdated: false,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}

/**
 * æ‰«ææ–‡ä»¶å¤¹
 */
export async function scanFolder(
  folderPath: string,
  options: ScanOptions = {}
): Promise<ScanResult> {
  const startTime = Date.now();
  const scanId = nanoid();

  // åˆå§‹åŒ–æ•°æ®åº“
  initializeDatabase();

  // åˆå§‹åŒ–åˆ†ç±»
  await initializeCategories();

  // [æ–°å¢ï¼] å¼ºåˆ¶åŒæ­¥å¹¶æ¿€æ´» MeiliSearch çš„å¯ç­›é€‰å­—æ®µé…ç½®
  await initializeIndex();

  const progress: ScanProgress = {
    phase: 'scanning',
    totalFiles: 0,
    processedFiles: 0,
    newFiles: 0,
    updatedFiles: 0,
    errors: [],
  };

  // åˆ›å»ºæ‰«æè®°å½•
  await db.insert(scanHistory).values({
    id: scanId,
    scanPath: folderPath,
    startedAt: new Date().toISOString(),
    status: 'running',
  });

  try {
    // æ‰«ææ–‡ä»¶
    progress.phase = 'scanning';
    options.onProgress?.(progress);

    const files = await getFilesRecursively(folderPath);
    console.log(`[Scanner] å…±å‘ç°å¾…å¤„ç†æ–‡ä»¶: ${files.length} ä¸ª`);
    progress.totalFiles = files.length;

    // è·å–ç°æœ‰æ–‡æ¡£
    const existingDocs = await db.select({
      id: documents.id,
      filePath: documents.filePath,
      fileHash: documents.fileHash,
    }).from(documents);

    const existingDocsMap = new Map(
      existingDocs.map(d => [d.filePath, { id: d.id, fileHash: d.fileHash }])
    );

    // å¤„ç†æ–‡ä»¶
    progress.phase = 'processing';

    for (const filePath of files) {
      progress.currentFile = filePath;
      options.onProgress?.(progress);

      const existingDoc = existingDocsMap.get(filePath);

      // å•æ–‡ä»¶ 120s è¶…æ—¶ä¿æŠ¤ï¼Œé˜²æ­¢å¤§æ–‡ä»¶æˆ– API è°ƒç”¨é˜»å¡æ•´ä¸ªæ‰«æ
      const FILE_TIMEOUT_MS = 120_000;
      const result = await Promise.race([
        processFile(filePath, existingDoc || undefined),
        new Promise<{ isNew: boolean; isUpdated: boolean; error: string }>((resolve) =>
          setTimeout(
            () => resolve({ isNew: false, isUpdated: false, error: 'å¤„ç†è¶…æ—¶ï¼ˆ>120sï¼‰ï¼Œå·²è·³è¿‡' }),
            FILE_TIMEOUT_MS
          )
        ),
      ]);


      progress.processedFiles++;

      if (result.isNew) {
        progress.newFiles++;
      } else if (result.isUpdated) {
        progress.updatedFiles++;
      }

      if (result.error) {
        progress.errors.push(`${filePath}: ${result.error}`);
      }

      options.onProgress?.(progress);
    }

    // æ›´æ–°æ‰«æè®°å½•
    progress.phase = 'completed';
    options.onProgress?.(progress);

    await db.update(scanHistory)
      .set({
        completedAt: new Date().toISOString(),
        totalFiles: progress.totalFiles,
        processedFiles: progress.processedFiles,
        newFiles: progress.newFiles,
        updatedFiles: progress.updatedFiles,
        status: 'completed',
      })
      .where(eq(scanHistory.id, scanId));

    return {
      scanId,
      totalFiles: progress.totalFiles,
      processedFiles: progress.processedFiles,
      newFiles: progress.newFiles,
      updatedFiles: progress.updatedFiles,
      errors: progress.errors,
      duration: Date.now() - startTime,
    };
  } catch (error) {
    // æ›´æ–°æ‰«æè®°å½•ä¸ºå¤±è´¥
    await db.update(scanHistory)
      .set({
        completedAt: new Date().toISOString(),
        status: 'failed',
      })
      .where(eq(scanHistory.id, scanId));

    throw error;
  }
}

/**
 * è·å–æ‰«æå†å²
 */
export async function getScanHistory(limit: number = 10): Promise<ScanHistory[]> {
  const history = await db.select()
    .from(scanHistory)
    .orderBy(scanHistory.startedAt)
    .limit(limit);

  return history.map(h => {
    const startedAt = new Date(h.startedAt);
    const completedAt = h.completedAt ? new Date(h.completedAt) : null;
    return {
      id: h.id,
      scanPath: h.scanPath,
      startedAt: isNaN(startedAt.getTime()) ? new Date() : startedAt,
      completedAt: completedAt && isNaN(completedAt.getTime()) ? null : completedAt,
      totalFiles: h.totalFiles || 0,
      processedFiles: h.processedFiles || 0,
      newFiles: h.newFiles || 0,
      updatedFiles: h.updatedFiles || 0,
      status: h.status as 'running' | 'completed' | 'failed',
    };
  });
}
